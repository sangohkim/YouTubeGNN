{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D3k2xkWqLXyZ",
        "y9sOe5fHZc9E"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Relationships About Videos in a YouTube Channel\n",
        "\n",
        " > Team 34. 20200549 Junha Jang, 20220112 Sangoh Kim, 20220025 Yejune Ko\n",
        "\n"
      ],
      "metadata": {
        "id": "2HiBqPQ9K3iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channel_name = '우고 - 군대가기 싫다'\n",
        "DATA_PATH = './drive/MyDrive/CS471_project/data'"
      ],
      "metadata": {
        "id": "_emY2koxjvxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup\n",
        "1. This project **uses GPU**. Therefore, please change the runtime type to `T4 GPU`.\n",
        "2. Unzip `CS471_project.zip` and place it right under `MyDrive`. After that, your Google drive's directory structure should looks like below. (If `processed/{channel_name}`, `raw/{channel_name}` directories not appear, then please manually add them in your Google drive)<br>\n",
        "data download link : https://drive.google.com/file/d/1pCVjnBQfEdjcNg2jatTdSBZdy6OfPso5/view?usp=drive_link\n",
        "```\n",
        "# This is formatted as dir\n",
        "├── MyDrive\n",
        "│   ├── CS471_project\n",
        "│   │   ├── data\n",
        "│   │   │   ├── raw\n",
        "│   │   │   │   ├──{Channel Name}.zip\n",
        "│   │   │   ├── processed\n",
        "│   │   │   │   ├──{Channel Name}  # empty dir\n",
        "│   │   │   ├── final\n",
        "│   │   │   │   ├──{Channel Name}  # empty dir\n",
        "```"
      ],
      "metadata": {
        "id": "D3k2xkWqLXyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tuvpZksLOl5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install required python packages via pip"
      ],
      "metadata": {
        "id": "y9sOe5fHZc9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asttokens==2.4.1\n",
        "!pip install colorama==0.4.6\n",
        "!pip install decorator\n",
        "!pip install exceptiongroup==1.2.1\n",
        "!pip install executing==2.0.1\n",
        "!pip install ipython\n",
        "!pip install jedi==0.19.1\n",
        "!pip install matplotlib-inline==0.1.7\n",
        "!pip install numpy\n",
        "!pip install pandas==2.0.3\n",
        "!pip install parso==0.8.4\n",
        "!pip install prompt-toolkit==3.0.43\n",
        "!pip install pure-eval==0.2.2\n",
        "!pip install Pygments==2.18.0\n",
        "!pip install python-dateutil\n",
        "!pip install pytz==2024.1\n",
        "!pip install scikit-network==0.32.1\n",
        "!pip install scipy==1.13.0\n",
        "!pip install six==1.16.0\n",
        "!pip install stack-data==0.6.3\n",
        "!pip install tqdm==4.66.4\n",
        "!pip install traitlets==5.14.3\n",
        "!pip install typing_extensions==4.11.0\n",
        "!pip install tzdata==2024.1\n",
        "!pip install wcwidth==0.2.13\n",
        "!pip install torch==2.3.0\n",
        "!pip install matplotlib\n",
        "!pip install torcheval==0.0.7\n",
        "!pip install scikit-learn==1.5.0"
      ],
      "metadata": {
        "id": "WVBxlq6mPJIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Tuple, List\n",
        "from tqdm import tqdm\n",
        "from sknetwork.clustering import Louvain\n",
        "from torcheval.metrics.functional import binary_f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "tjzi7ApsR2c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Generating Graph Dataset"
      ],
      "metadata": {
        "id": "svRkGfqoPQJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code cell will unzip the raw YouTube Studio data. It will take about **15 minutes** (depends on network connection)"
      ],
      "metadata": {
        "id": "L2tE7WeVRm8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_dir = f\"/content/drive/My Drive/CS471_project/data/raw/{channel_name}.zip\"\n",
        "zip_ref = zipfile.ZipFile(zip_dir, 'r')\n",
        "\n",
        "dir = \"/content/drive/My Drive/CS471_project/data/raw/\"\n",
        "zip_ref.extractall(dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "-vTUeTFJP8G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below `YTGraphGenerator` extracts nodes, edges information from above unzipped raw data and store those graph dataset under `data/processed` directory"
      ],
      "metadata": {
        "id": "trf1V2JeShy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YTGraphGenerator:\n",
        "    def __init__(self, channel_name):\n",
        "        self.channel_name = channel_name\n",
        "        self.channel_path = DATA_PATH + '/raw/' + channel_name\n",
        "        self.video_path = self.channel_path + '/videos'\n",
        "\n",
        "        self.graph_node_id_set = set()\n",
        "        self.graph_edge_dict = {}\n",
        "\n",
        "        self.graph_node_info_dict = {}\n",
        "\n",
        "        self.modified_graph_edge_dict = {}\n",
        "        self.cnt = 0\n",
        "\n",
        "        self.graph_edge_list = []\n",
        "\n",
        "    def process_video_list(self):\n",
        "        print(\"Step 2-1: process_video_list\")\n",
        "\n",
        "        # Get a list of all files and directories in the specified directory\n",
        "        self.video_id_list = os.listdir(self.video_path)\n",
        "        self.origin_video_id_list = copy.deepcopy(self.video_id_list)\n",
        "        print(\"Number of video : \", len(self.video_id_list))\n",
        "        print()\n",
        "\n",
        "    def process_yt_related(self):\n",
        "        print(\"Step 2-2: process_yt_related\")\n",
        "\n",
        "        for video_id in tqdm(self.video_id_list, desc=\"Processing\"):\n",
        "            if video_id == '.DS_Store' or len(video_id) == 0:\n",
        "                continue\n",
        "            # print(\"video id: \", video_id)\n",
        "            self.graph_node_id_set.add(video_id)\n",
        "            directory_path = self.video_path + '/' + video_id + '/VIDEO/표 데이터.csv'\n",
        "            with open(directory_path, mode='r', encoding='utf-8') as file:\n",
        "                # 증가한 구독자[48], 좋아요[27], 조회수[1], 노출수[52], 노출클릭율[51], 수익[42], 댓글[23], 평균 조회율[49]\n",
        "                csv_reader = csv.reader(file)\n",
        "                header = next(csv_reader)\n",
        "                for row in csv_reader:\n",
        "                    self.graph_node_info_dict[video_id] = [row[48], row[27], row[1], row[52], row[51], row[42], row[23], row[49]]\n",
        "                    break\n",
        "\n",
        "            directory_path = self.video_path + '/' + video_id + '/VIDEO/총계.csv'\n",
        "            with open(directory_path, mode='r', encoding='utf-8') as file:\n",
        "                csv_reader = csv.reader(file)\n",
        "                header = next(csv_reader)\n",
        "                for row in csv_reader:\n",
        "                    self.graph_node_info_dict[video_id].append(row[0])\n",
        "                    break\n",
        "\n",
        "            # Open the CSV file\n",
        "            directory_path = self.video_path + '/' + video_id + '/TRAFFIC_SOURCE_DETAIL__YT_RELATED/표 데이터.csv'\n",
        "            related_information = []\n",
        "            with open(directory_path, mode='r', encoding='utf-8') as file:\n",
        "                # Create a CSV reader object\n",
        "                csv_reader = csv.reader(file)\n",
        "\n",
        "                # Iterate over each row in the CSV file\n",
        "                for row in csv_reader:\n",
        "                    if len(row) < 4:\n",
        "                        continue\n",
        "                    # Each row is a list where each element represents a column value\n",
        "                    # print(row)\n",
        "                    related_video_id = row[0][11:]\n",
        "                    related_video_view = row[3]\n",
        "                    related_information.append([video_id, related_video_id, related_video_view])\n",
        "                    # print(\"related video id: \", related_video_id, \"related video view: \", related_video_view)\n",
        "\n",
        "                    if (not related_video_view == '조회수') and len(related_video_view) > 0 and len(related_video_id) > 0 and int(related_video_view) > 1 and (related_video_id in self.origin_video_id_list):\n",
        "                        self.graph_node_id_set.add(related_video_id)\n",
        "                        self.graph_edge_dict[(video_id, related_video_id)] = int(related_video_view)\n",
        "                        # print(\"related video id : \", related_video_id)\n",
        "                        # print(\"len related vid id : \", len(related_video_id))\n",
        "                        # print(\"related video view : \", related_video_view)\n",
        "\n",
        "\n",
        "        csv_file_path = DATA_PATH + '/processed/' + self.channel_name + '/node_information.csv'\n",
        "        with open(csv_file_path, 'w', newline='') as file:\n",
        "            csv_writer = csv.writer(file)\n",
        "\n",
        "            csv_writer.writerow(['video_id', 'subscriber', 'like', 'view', 'exposure', 'exposure_click_rate', 'revenue', 'comment', 'average_view_rate', 'uploaded_time'])\n",
        "            for row in self.graph_node_info_dict.items():\n",
        "                line = [row[0]] + row[1]\n",
        "                csv_writer.writerow(line)\n",
        "\n",
        "        print(\"Number of nodes : \", len(self.graph_node_id_set))\n",
        "        print(\"Number of edges : \", len(self.graph_edge_dict))\n",
        "        print()\n",
        "\n",
        "    def integrate_edges(self):\n",
        "        print(\"Step 2-3: integrate_edges\")\n",
        "\n",
        "        # integrate in-degree and out-degree\n",
        "        for edge in self.graph_edge_dict:\n",
        "            to_node_id = edge[0]\n",
        "            from_node_id = edge[1]\n",
        "            weight1 = self.graph_edge_dict[edge]\n",
        "            weight2 = 0\n",
        "            # print(\"to_node_id: \", to_node_id, \"from_node_id: \", from_node_id)\n",
        "            if (from_node_id, to_node_id) in self.graph_edge_dict:\n",
        "                self.cnt += 1\n",
        "                # print(\"from_node_id: \", from_node_id, \"to_node_id: \", to_node_id)\n",
        "                weight2 = self.graph_edge_dict[(from_node_id, to_node_id)]\n",
        "                # del graph_edge_set[(from_node_id, to_node_id)]\n",
        "\n",
        "            new_weight = (math.log(weight1 + 1) + math.log(weight2 + 1)) / 2\n",
        "            self.modified_graph_edge_dict[(to_node_id, from_node_id)] = new_weight\n",
        "\n",
        "            # del graph_edge_set[edge]\n",
        "\n",
        "        print(\"Number of edges originally had edges in both direction  : \", self.cnt)\n",
        "        print()\n",
        "\n",
        "    def process_yt_related2(self):\n",
        "        # Data for EvolveGCN\n",
        "        print(\"Step 2-4: process_yt_related2\")\n",
        "\n",
        "        video_dict = {}\n",
        "        for video_idx, video_id in enumerate(self.video_id_list):\n",
        "            video_dict[video_id] = video_idx + 1\n",
        "\n",
        "        for video_id in tqdm(self.video_id_list, desc=\"Processing\"):\n",
        "            if video_id == '.DS_Store' or len(video_id) == 0:\n",
        "                continue\n",
        "            # print(\"video id: \", video_id)\n",
        "            # self.graph_node_id_set.add(video_id)\n",
        "            directory_path = self.video_path + '/' + video_id + '/TRAFFIC_SOURCE_DETAIL__YT_RELATED/차트 데이터.csv'\n",
        "            with open(directory_path, mode='r', encoding='utf-8') as file:\n",
        "                csv_reader = csv.reader(file)\n",
        "                header = next(csv_reader)\n",
        "\n",
        "                prev_source = prev_target = None\n",
        "                for row in csv_reader:\n",
        "                    source = video_dict.get(video_id, 0)\n",
        "                    target = video_dict.get(row[1].split(\"YT_RELATED.\")[-1], 0)\n",
        "                    if source == 0 or target == 0:\n",
        "                        continue\n",
        "\n",
        "                    if prev_source != source or prev_target != target:\n",
        "                        weight = 0\n",
        "                    weight += int(row[4])\n",
        "                    time = row[0]\n",
        "\n",
        "                    # if weight > 0:\n",
        "                    self.graph_edge_list.append([source, target, weight, time])\n",
        "\n",
        "                    prev_source = source\n",
        "                    prev_target = target\n",
        "\n",
        "        def parse_date(date_str):\n",
        "            return datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n",
        "\n",
        "        def calculate_date_diff(older_date, newer_date):\n",
        "            return (newer_date - older_date).days\n",
        "\n",
        "        self.graph_edge_list.sort(key=lambda x: parse_date(x[3]))\n",
        "        oldest_date = parse_date(self.graph_edge_list[0][3])\n",
        "\n",
        "        for row in self.graph_edge_list:\n",
        "            current_date = parse_date(row[3])\n",
        "            date_diff = calculate_date_diff(oldest_date, current_date)\n",
        "            row[3] = date_diff\n",
        "\n",
        "        # print(self.graph_edge_list[-1][3])\n",
        "\n",
        "        csv_file_path = DATA_PATH + '/processed/' + self.channel_name + '/egcn.csv'\n",
        "        with open(csv_file_path, 'w', newline='') as file:\n",
        "            csv_writer = csv.writer(file)\n",
        "\n",
        "            csv_writer.writerow(['source', 'target', 'weight', 'time'])\n",
        "            for row in self.graph_edge_list:\n",
        "                csv_writer.writerow(row)\n",
        "\n",
        "\n",
        "    def write_graph(self):\n",
        "        print(\"Step 2-4: write_graph\")\n",
        "\n",
        "        # download nodes, edges\n",
        "        # dir = '/content/drive/MyDrive/CS471_project/Example_YT_Dataset/nodes.csv'\n",
        "        dir = DATA_PATH + '/processed/' + self.channel_name + '/nodes.csv'\n",
        "\n",
        "        with open(dir, 'w', newline='') as file:\n",
        "            # Create a CSV writer object\n",
        "            csv_writer = csv.writer(file)\n",
        "\n",
        "            # Write each row of data to the CSV file\n",
        "            for node in self.graph_node_id_set:\n",
        "                csv_writer.writerow([node])\n",
        "\n",
        "        # dir = '/content/drive/MyDrive/CS471_project/Example_YT_Dataset/edges.csv'\n",
        "        dir = DATA_PATH + '/processed/' + self.channel_name + '/edges.csv'\n",
        "\n",
        "        with open(dir, 'w', newline='') as file:\n",
        "            # Create a CSV writer object\n",
        "            csv_writer = csv.writer(file)\n",
        "\n",
        "            # Write each row of data to the CSV file\n",
        "            for edge in self.modified_graph_edge_dict:\n",
        "                line = [edge[0], edge[1], self.modified_graph_edge_dict[edge]]\n",
        "                csv_writer.writerow(line)\n",
        "        print()"
      ],
      "metadata": {
        "id": "NbgEGizKRsI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2 = YTGraphGenerator(channel_name)\n",
        "step2.process_video_list()\n",
        "step2.process_yt_related()\n",
        "step2.integrate_edges()\n",
        "step2.write_graph()"
      ],
      "metadata": {
        "id": "6oVfXcvKXDRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Analyzing the Graph with Louvain Method\n",
        "The code in this section applies Louvain Method to our graph dataset, compute modularity, and store the clustering result."
      ],
      "metadata": {
        "id": "hkJ77AWpXJTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_normalized_cut: louvain method를 통해 clustering 된 그래프의 normalized cut 값을 계산해보기 위해서 정의\n",
        "def compute_normalized_cut(adj: sp.spmatrix,  # spmatrix: scipy.sparse의 모든 sparse matrix의 super class\n",
        "                           labels: np.ndarray,\n",
        "                           num_communities: int) -> Tuple[np.ndarray, np.float_]:\n",
        "    adj_coo = adj.tocoo()\n",
        "    edgeList = list(zip(adj_coo.row, adj_coo.col, adj_coo.data))  # edge list 만들 때 coo_matrix 형태가 유용\n",
        "\n",
        "    cut = np.zeros(num_communities)\n",
        "    vol = np.zeros(num_communities)\n",
        "\n",
        "    for (u, v, w) in edgeList:\n",
        "        # 서로 연결된 모든 조합에 대해서 deg 정보 계산.\n",
        "        # 만약 속하는 cluster가 다르다면 cut 정보 계산.\n",
        "        vol[labels[u]] += w  # deg(cluster of u)를 계산하는 과정\n",
        "        if labels[u] != labels[v]:\n",
        "            cut[labels[u]] += w\n",
        "\n",
        "    normalized_cuts = cut / vol\n",
        "\n",
        "    return normalized_cuts, np.sum(normalized_cuts)\n",
        "\n",
        "# clustering evaluation을 위한 modularity 계산 함수\n",
        "def compute_modularity(adj: sp.spmatrix,\n",
        "                       labels: np.ndarray) -> float:\n",
        "    adj_coo = adj.tocoo()\n",
        "    edgeList = list(zip(adj_coo.row, adj_coo.col, adj_coo.data))\n",
        "\n",
        "    edgeList_np = np.array(edgeList)\n",
        "\n",
        "    modularity = 0.0\n",
        "    m_tot = adj.sum()\n",
        "\n",
        "    for (u, v, w) in edgeList:\n",
        "        if labels[u] == labels[v]:\n",
        "            k_u = edgeList_np[edgeList_np[:, 0] == u][:, 2].sum()\n",
        "            k_v = edgeList_np[edgeList_np[:, 0] == v][:, 2].sum()\n",
        "            modularity += adj[u, v] - k_u * k_v / m_tot\n",
        "\n",
        "    return modularity / m_tot"
      ],
      "metadata": {
        "id": "iZFXoWXvXZyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YTClustering:\n",
        "    def __init__(self, channel_name):\n",
        "        print(\"Step 3-1 : YTClustering\")\n",
        "\n",
        "        self.base_path = DATA_PATH + '/processed/' + channel_name\n",
        "\n",
        "        self.nodes_str = pd.read_csv(f'{self.base_path}/node_information.csv')['video_id'].values.squeeze()\n",
        "\n",
        "        self.edges_csv = open(f'{self.base_path}/edges.csv')\n",
        "        self.edges_csv_reader = csv.reader(self.edges_csv)\n",
        "\n",
        "        self.generator()\n",
        "        self.applier()\n",
        "\n",
        "        print()\n",
        "\n",
        "    def generator(self):\n",
        "        nodes_list = self.nodes_str.tolist()\n",
        "        nodes_num = [i for i in range(len(nodes_list))]\n",
        "\n",
        "        node_dict = dict(zip(nodes_list, nodes_num))\n",
        "\n",
        "        num_to_node_dict = dict(zip(nodes_num, nodes_list))\n",
        "\n",
        "        # edge 정보를 coo_matrix로 표현\n",
        "        data, row, col = [], [], []\n",
        "\n",
        "        for (src, dst, weight) in self.edges_csv_reader:\n",
        "            src_num, dst_num = node_dict[src], node_dict[dst]\n",
        "            data.append(float(weight))\n",
        "            row.append(src_num)\n",
        "            col.append(dst_num)\n",
        "        data = np.array(data)\n",
        "        row = np.array(row)\n",
        "        col = np.array(col)\n",
        "\n",
        "        print(f\"# of nodes: {len(nodes_num)}\")\n",
        "        print(f\"# of edges: {data.shape}\")\n",
        "\n",
        "        self.sparse_coo_matrix = sp.coo_matrix((data, (row, col)), shape=(len(nodes_num), len(nodes_num)))\n",
        "        self.sparse_csr_matrix = sp.csr_matrix(self.sparse_coo_matrix)\n",
        "\n",
        "    def applier(self):\n",
        "        louvain = Louvain()\n",
        "        labels = louvain.fit_predict(self.sparse_csr_matrix)\n",
        "\n",
        "        unique_communities, counts = np.unique(labels, return_counts = True)\n",
        "        num_communities = len(unique_communities)\n",
        "        print(f\"Number of total communities: {num_communities}\")\n",
        "        print(f\"Number of nodes in each community: {counts}\")\n",
        "\n",
        "        normalized_cuts, NC = compute_normalized_cut(self.sparse_csr_matrix, labels, num_communities)\n",
        "        Q = compute_modularity(self.sparse_csr_matrix, labels)\n",
        "\n",
        "        print(f\"Normalized cut of each cluster: {normalized_cuts}\")\n",
        "        print(f\"Graph normalized cut: {NC:.4f}\")\n",
        "        print(f\"Modularity: {Q:.4f}\")\n",
        "\n",
        "        with open(self.base_path + '/cluster.csv', 'w', newline='') as file:\n",
        "            # Create a CSV writer object\n",
        "            csv_writer = csv.writer(file)\n",
        "            csv_writer.writerow(labels)  # nodes.csv의 node 순서와 동일하게 매핑됨."
      ],
      "metadata": {
        "id": "Xvld16ROXmWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step3 = YTClustering(channel_name)"
      ],
      "metadata": {
        "id": "nQpk08NVXzGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Analyzing Clustered Data\n",
        "The cluster labels obtained through the Louvain method were used to summarize the characteristics of each cluster based on studio data.\n",
        "\n",
        "\n",
        "We have created csv report for all labels in the studio. It will be created in `/data/final/{channel_name}`. Since there are too many labels, we have selected 8 main features `'increased subscriber', 'like', 'view', 'exposure', 'exposure_click_rate', 'revenue', 'comment', 'average_view_rate', 'uploaded_time'`. Result about the 8 features is summarized in `/data/final/{channel_name}/VIDEO.csv`."
      ],
      "metadata": {
        "id": "RmNuC25ZZqz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DIMESIONS` are the features that are related to each video. Select the features from the following list and add them to the code.<br>\n",
        "(`DAY` feature takes more than an hour on colab)\n",
        "\n",
        "```\n",
        "DIMENSIONS = [\n",
        "    'VIDEO', 'PLAYER_APP_TYPE', 'CAPTION_LANGUAGE', 'DEVICE_OS_TYPE', 'VIEWER_GENDER', 'INFO_CARD_TYPE',\n",
        "    'VIEWER_AGE', 'SHARING_SERVICE', 'DAY', 'CITY', 'PLAYBACK_LOCATION_TYPE',\n",
        "    'IS_CROSS_LANGUAGE', 'EMBEDDED_PLAYER_MODE', 'ADTYPES', 'EARNINGS_SOURCE_ALL', 'SUBSCRIBED_TO_UPLOADER_STATE',\n",
        "    'LOYALTY_STATE', 'VIDEO_METADATA_LANGUAGE', 'TRAFFIC_SOURCE_TYPE', 'TRANSACTION_BUSINESS_MODEL',\n",
        "    'SUBSCRIPTION_SOURCE_TYPE', 'DEVICE_PLATFORM_TYPE', 'ENDSCREEN_ELEMENT_TYPE', 'ENDSCREEN_ELEMENT_ID',\n",
        "    'INFO_CARD_ID', 'CREATOR_CONTENT_TYPE', 'POST', 'COUNTRY'\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "-o2n9yw1bWJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIMENSIONS = [\n",
        "    'VIDEO', 'VIEWER_GENDER', 'VIEWER_AGE', 'ADTYPES',\n",
        "    'EARNINGS_SOURCE_ALL', 'SUBSCRIBED_TO_UPLOADER_STATE',\n",
        "    'SUBSCRIPTION_SOURCE_TYPE', 'DEVICE_PLATFORM_TYPE', 'CREATOR_CONTENT_TYPE'\n",
        "]"
      ],
      "metadata": {
        "id": "-GCaKAZmZtny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YTGraphAnalyzer:\n",
        "    def __init__(self, channel_name):\n",
        "        self.processed_dimension = []\n",
        "\n",
        "        print(\"Step 4-1 : process_dimension\")\n",
        "        self.process_dimension()\n",
        "        print()\n",
        "\n",
        "        print(\"Step 4-2 : dataframe_generator\")\n",
        "        for dimension in tqdm(DIMENSIONS, desc=\"Processing\"):\n",
        "            self.dataframe_generator(channel_name, dimension)\n",
        "        print()\n",
        "\n",
        "        print(\"Step 4-3 : video_dataframe_processor\")\n",
        "        self.video_dataframe_processor(channel_name)\n",
        "        print()\n",
        "\n",
        "    def process_dimension(self):\n",
        "        # 기본 경로 설정\n",
        "        base_directory_path = DATA_PATH + '/raw/' + channel_name + '/videos'\n",
        "\n",
        "        # 각 DIMENSION의 최대 항목 수를 저장할 딕셔너리\n",
        "        max_items_per_dimension = {dimension: 0 for dimension in DIMENSIONS}\n",
        "        min_items_per_dimension = {dimension: 3 for dimension in DIMENSIONS}\n",
        "\n",
        "        # base_directory_path 경로 내의 모든 폴더를 순회\n",
        "        for item in tqdm(os.listdir(base_directory_path), desc=\"Processing\"):\n",
        "            item_path = os.path.join(base_directory_path, item)\n",
        "            # print(item_path)\n",
        "            # 폴더인지 확인\n",
        "            if os.path.isdir(item_path):\n",
        "                # DIMENSIONS 리스트 내의 각 폴더에 대해\n",
        "                for dimension in DIMENSIONS:\n",
        "                    dimension_path = os.path.join(item_path, dimension)\n",
        "                    # 폴더 내 항목 수 계산\n",
        "                    # 해당 경로가 실제로 존재하는 폴더인지 확인\n",
        "                    item_count = len(os.listdir(dimension_path)) if os.path.isdir(dimension_path) else 0\n",
        "\n",
        "                    # 현재 항목 수가 이전 최댓값보다 큰 경우 업데이트\n",
        "                    if item_count > max_items_per_dimension[dimension]:\n",
        "                        max_items_per_dimension[dimension] = item_count\n",
        "                    if item_count < min_items_per_dimension[dimension]:\n",
        "                        min_items_per_dimension[dimension] = item_count\n",
        "\n",
        "        # max와 min이 같은 dimension들을 찾아서 출력\n",
        "        for dimension in DIMENSIONS:\n",
        "            if max_items_per_dimension[dimension] == min_items_per_dimension[dimension]:\n",
        "                self.processed_dimension.append(dimension)\n",
        "                # print(f\"{dimension}: {max_items_per_dimension[dimension]}\")\n",
        "\n",
        "        print(self.processed_dimension)\n",
        "\n",
        "    def dataframe_generator(self, channel_name, dimension, type=None):\n",
        "        # 기본 경로 설정\n",
        "        videos_directory_path = os.path.join(DATA_PATH, 'raw', channel_name, 'videos')\n",
        "        cluster_directory_path = os.path.join(DATA_PATH, 'processed', channel_name, 'cluster.csv')\n",
        "\n",
        "        # clusters.csv에서 클러스터 정보 로드\n",
        "        clusters = pd.read_csv(cluster_directory_path, header=None).squeeze()  # Series로 변환\n",
        "\n",
        "        video_id_list = os.listdir(videos_directory_path)\n",
        "\n",
        "        # 클러스터별 데이터 프레임을 저장할 딕셔너리\n",
        "        cluster_data_frames = {}\n",
        "\n",
        "        for video_id, cluster in zip(video_id_list, clusters):\n",
        "            # print(video_id, cluster)\n",
        "\n",
        "            video_directory_path = os.path.join(videos_directory_path, video_id, dimension, '표 데이터.csv')\n",
        "            if os.path.exists(video_directory_path):\n",
        "                # CSV 파일 로드\n",
        "                df = pd.read_csv(video_directory_path, header=None)  # 헤더 없이 로드\n",
        "\n",
        "                # 해당 클러스터의 리스트에 데이터 프레임 추가\n",
        "                if cluster not in cluster_data_frames:\n",
        "                    cluster_data_frames[cluster] = [df]\n",
        "                else:\n",
        "                    cluster_data_frames[cluster].append(df)\n",
        "\n",
        "        # 클러스터별로 평균값 계산 및 저장\n",
        "        for cluster, data_frames in cluster_data_frames.items():\n",
        "            if not data_frames:\n",
        "                print(f\"클러스터 {cluster}에 대한 데이터가 없습니다.\")\n",
        "                continue\n",
        "\n",
        "            # 첫 번째 데이터 프레임을 기준으로 삼음\n",
        "            base_df = data_frames[0].copy()\n",
        "\n",
        "            # 나머지 데이터 프레임들과 비교하여 같은 위치의 셀 평균 계산\n",
        "            for i in range(1, len(data_frames)):\n",
        "                current_df = data_frames[i]\n",
        "                for row in range(min(len(base_df), len(current_df))):\n",
        "                    for col in range(min(len(base_df.columns), len(current_df.columns))):\n",
        "                        base_value = pd.to_numeric(base_df.iloc[row, col], errors='coerce')\n",
        "                        current_value = pd.to_numeric(current_df.iloc[row, col], errors='coerce')\n",
        "                        if pd.notnull(current_value):\n",
        "                            if pd.notnull(base_value):\n",
        "                                base_df.iloc[row, col] = np.mean([base_value, current_value])\n",
        "                            else:\n",
        "                                base_df.iloc[row, col] = current_value\n",
        "\n",
        "            # 평균값을 담은 데이터 프레임을 CSV로 저장\n",
        "            output_path = os.path.join(DATA_PATH, 'final', channel_name, f'{dimension}{cluster}.csv')\n",
        "            base_df.to_csv(output_path, index=False, header=False)\n",
        "            # print(f\"클러스터 {cluster}의 결과가 {output_path}에 저장되었습니다.\")\n",
        "\n",
        "    def video_dataframe_processor(self, channel_name, dimension=\"VIDEO\", type=None):\n",
        "        # 모든 데이터프레임을 아래로 합치기\n",
        "        df_list = []\n",
        "        for cluster_index in tqdm(range(3), desc=\"Processing\"):\n",
        "            df_directory_path = os.path.join(DATA_PATH, 'final', channel_name, f'{dimension}{cluster_index}.csv')\n",
        "            df_list.append(pd.read_csv(df_directory_path, header=0))\n",
        "        df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "        # 특정 행들만 선택\n",
        "        df = df.loc[:, ['조회수', '추가된 댓글 수', '좋아요', '예상 수익 (KRW)', '구독자', '평균 조회율 (%)', '노출 클릭률 (%)', '노출수']]\n",
        "\n",
        "        # 행 이름 변경\n",
        "        # df.rename(index={\n",
        "        #     '조회수': 'Views',\n",
        "        #     '추가된 댓글 수': 'Comments Added',\n",
        "        #     '좋아요': 'Likes',\n",
        "        #     '예상 수익 (KRW)': 'Estimated Revenue (KRW)',\n",
        "        #     '구독자': 'Subscribers',\n",
        "        #     '평균 조회율 (%)': 'Average Watch Percentage (%)',\n",
        "        #     '노출 클릭률 (%)': 'Impression Click-Through Rate (%)',\n",
        "        #     '노출수': 'Impressions'\n",
        "        # }, inplace=True)\n",
        "\n",
        "        # 출력 경로 설정\n",
        "        output_path = os.path.join(DATA_PATH, 'final', channel_name, f'{dimension}.csv')\n",
        "\n",
        "        # 새로운 CSV 파일로 저장\n",
        "        df.to_csv(output_path, index=True, header=True)"
      ],
      "metadata": {
        "id": "PXaYhGUIbZEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code cell will take about 2 minutes."
      ],
      "metadata": {
        "id": "reR3TDtZb9BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step4 = YTGraphAnalyzer(channel_name)\n",
        "step4.video_dataframe_processor(channel_name)"
      ],
      "metadata": {
        "id": "5A2Z-Kybbd3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Applying GNNs for Link Prediction"
      ],
      "metadata": {
        "id": "o9Bh1yZUcFe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        torch.mps.manual_seed(seed)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # CuDNN 라이브러리의 최적화된 값\n",
        "# 재현가능한 결과가 나오도록 함. 성능의 손실이 약간 있을 수 있음\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms(True)  # 재현가능한 결과가 나오도록 함.\n",
        "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":16:8\"\n",
        "\n",
        "\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "9-0bStBDcIJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_device() -> torch.device:\n",
        "    # device 설정\n",
        "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        return torch.device('mps')\n",
        "    elif torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "device = set_device()\n",
        "if device == torch.device('mps'):\n",
        "    FLOAT_TYPE = torch.float32\n",
        "else:\n",
        "    FLOAT_TYPE = torch.float64\n",
        "print(f\"Using {device}\")"
      ],
      "metadata": {
        "id": "Gba1U2QndRLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = f'{DATA_PATH}/processed'"
      ],
      "metadata": {
        "id": "knHNHjWndTi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_information_df = pd.read_csv(f'{base_path}/{channel_name}/node_information.csv')\n",
        "\n",
        "nodes_str = node_information_df['video_id'].values.squeeze()\n",
        "\n",
        "# node 별로 번호를 할당\n",
        "nums = [i for i in range(len(nodes_str))]\n",
        "node2num_dict = dict(zip(nodes_str, nums))\n",
        "\n",
        "node_features = node_information_df.drop(['video_id', 'uploaded_time'], axis=1).values\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "node_features = std_scaler.fit_transform(node_features)\n",
        "node_features = torch.tensor(node_features, dtype=FLOAT_TYPE)"
      ],
      "metadata": {
        "id": "jL05nEyZdckT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unix_timestamp(ts: str) -> float:\n",
        "    date = np.datetime64(ts, 's')\n",
        "    timestamp = (date - np.datetime64('1970-01-01T00:00:00', 's')) / np.timedelta64(1, 's')\n",
        "\n",
        "    return timestamp\n",
        "\n",
        "\n",
        "edges_tot = []  # src_num, dst_num, weights, uploaded_unix_timestamp\n",
        "with open(f'{base_path}/{channel_name}/edges.csv') as edges_f:\n",
        "    rdr = csv.reader(edges_f)\n",
        "    for src_str, dst_str, weight_str in rdr:\n",
        "        src_str, dst_str, weight_str = src_str.strip(), dst_str.strip(), weight_str.strip()\n",
        "        src_timestamp = get_unix_timestamp(node_information_df[node_information_df['video_id'] == src_str]['uploaded_time'].item())\n",
        "        dst_timestamp = get_unix_timestamp(node_information_df[node_information_df['video_id'] == dst_str]['uploaded_time'].item())\n",
        "        src_num, dst_num = node2num_dict[src_str], node2num_dict[dst_str]\n",
        "        edges_tot.append([src_num, dst_num, float(weight_str.strip()), src_timestamp if src_timestamp > dst_timestamp else dst_timestamp])\n",
        "\n",
        "edges_tot = np.array(edges_tot, dtype=np.float64)\n",
        "edges_int = edges_tot[:, [0, 1, 3]].astype(np.int64)\n",
        "edges_int = edges_int[np.lexsort([edges_int[:, 1], edges_int[:, 0]])]\n",
        "edges_int = torch.tensor(edges_int, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "D3tWKGD8dfDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_degree(node_features, edges: torch.Tensor) -> torch.Tensor:\n",
        "    adj = torch.zeros(node_features.shape[0], node_features.shape[0], dtype=FLOAT_TYPE)\n",
        "    adj[edges[:, 0], edges[:, 1]] = 1.0\n",
        "\n",
        "    degree = adj.sum(dim=1)\n",
        "    degree[degree == 0] = 1e-6\n",
        "\n",
        "    return degree"
      ],
      "metadata": {
        "id": "5WXp3Z3ddhnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_str_datetime(ts: int) -> str:\n",
        "    return datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "def split_index(train_size: float = 0.6, valid_size: float = 0.2, test_size: float = 0.2) -> Tuple[torch.Tensor]:\n",
        "    train_idx = int(edges_int.shape[0] * train_size)\n",
        "    msg_idx = int(edges_int.shape[0] * train_size * 0.8)\n",
        "    valid_idx = int(edges_int.shape[0] * (train_size + valid_size))\n",
        "    edges_ts_asc = edges_int[:, 2].sort()[0]\n",
        "\n",
        "    start = get_str_datetime(edges_ts_asc[0].item())\n",
        "    msg_end_ts = edges_ts_asc[msg_idx].item()\n",
        "    msg_end = get_str_datetime(msg_end_ts)\n",
        "    train_end_ts = edges_ts_asc[train_idx].item()\n",
        "    train_end = get_str_datetime(edges_ts_asc[train_idx].item())\n",
        "    valid_end_ts = edges_ts_asc[valid_idx].item()\n",
        "    valid_end = get_str_datetime(edges_ts_asc[valid_idx].item())\n",
        "    test_end = get_str_datetime(edges_ts_asc[-1].item())\n",
        "\n",
        "    train_indices_msg = torch.where(edges_int[:, 2] < msg_end_ts)[0]\n",
        "    train_indices_pred = torch.where((edges_int[:, 2] >= msg_end_ts) & (edges_int[:, 2] < train_end_ts))[0]\n",
        "    val_indices = torch.where((edges_int[:, 2] >= train_end_ts) & (\n",
        "        edges_int[:, 2] < valid_end_ts))[0]\n",
        "    test_indices = torch.where(edges_int[:, 2] >= valid_end_ts)[0]\n",
        "\n",
        "    print(f'Training (message passing & aggregation). ({train_indices_msg.shape[0]} edges): {start} ~ {msg_end}')\n",
        "    print(f'Training (prediction). ({train_indices_pred.shape[0]} edges): {msg_end} ~ {train_end}')\n",
        "    print(f'Validation.   ({val_indices.shape[0]} edges): {train_end} ~ {valid_end}')\n",
        "    print(f'Test.        ({test_indices.shape[0]} edges): {valid_end} ~ {test_end}')\n",
        "\n",
        "    return train_indices_msg, train_indices_pred, val_indices, test_indices\n",
        "\n",
        "\n",
        "train_indices_msg, train_indices_pred, val_indices, test_indices = split_index()"
      ],
      "metadata": {
        "id": "JiaSf9kEdkbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSageLayer(nn.Module):\n",
        "    def __init__(self, dim_in: int, dim_out: int, p: float, agg_type: str):\n",
        "        super(GraphSageLayer, self).__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.agg_type = agg_type\n",
        "        self.device = device\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        if self.agg_type == 'gcn':\n",
        "            self.weight = nn.Linear(\n",
        "                self.dim_in, self.dim_out, bias=False, dtype=FLOAT_TYPE)  # W_l\n",
        "            self.bias = nn.Linear(self.dim_in, self.dim_out,\n",
        "                                  bias=False, dtype=FLOAT_TYPE)  # B_l\n",
        "        elif self.agg_type == 'mean':\n",
        "            self.weight = nn.Linear(\n",
        "                2 * self.dim_in, self.dim_out, bias=False, dtype=FLOAT_TYPE)  # W_l\n",
        "        elif self.agg_type == 'maxpool':\n",
        "            self.linear_pool = nn.Linear(\n",
        "                # W_{pool}, b\n",
        "                self.dim_in, self.dim_in, bias=True, dtype=FLOAT_TYPE)\n",
        "            self.weight = nn.Linear(\n",
        "                2 * self.dim_in, self.dim_out, bias=False, dtype=FLOAT_TYPE)  # W_l\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unknown aggregation type: {self.agg_type}\")\n",
        "\n",
        "\n",
        "    def forward(self, feat: torch.Tensor, edge: torch.Tensor, degree: torch.Tensor) -> torch.Tensor:\n",
        "        if self.agg_type == 'gcn':\n",
        "            # aggregation\n",
        "            X = torch.zeros(feat.shape[0], feat.shape[1],\n",
        "                            dtype=FLOAT_TYPE).to(device)\n",
        "            X.index_add_(0, edge[:, 0], feat[edge[:, 1]])\n",
        "            X /= degree.reshape(feat.shape[0], -1)\n",
        "            # NN\n",
        "            out = self.act(self.weight(X) + self.bias(feat))\n",
        "            # normalization\n",
        "            out = F.normalize(out, p=2.0, dim=1)\n",
        "            return out\n",
        "        elif self.agg_type == 'mean':\n",
        "            # aggregation\n",
        "            X = torch.zeros(feat.shape[0], feat.shape[1],\n",
        "                            dtype=FLOAT_TYPE).to(device)\n",
        "            X.index_add_(0, edge[:, 0], feat[edge[:, 1]])\n",
        "            X /= degree.reshape(feat.shape[0], -1)\n",
        "            # concatenating self transformation\n",
        "            X = torch.cat([X, feat], dim=1).to(device)\n",
        "            # NN\n",
        "            out = self.act(self.weight(X))\n",
        "            # normalization\n",
        "            out = F.normalize(out, p=2.0, dim=1)\n",
        "            return out\n",
        "        elif self.agg_type == 'maxpool':\n",
        "            # self.linear_pool의 bias=True\n",
        "            pooled_feat = self.act(self.linear_pool(feat))\n",
        "            X = torch.zeros(\n",
        "                pooled_feat.shape[0], pooled_feat.shape[1], dtype=FLOAT_TYPE).to(device)\n",
        "            # scatter_reduce와 index_add 차이 주의\n",
        "            X.scatter_reduce_(\n",
        "                0, edge[:, 0].reshape(-1, 1).repeat(1, feat.shape[1]), pooled_feat[edge[:, 1]], reduce='amax', include_self=False)\n",
        "            # forwarded feat가 아닌 원래 feat과 cat해야함.\n",
        "            X = torch.cat([X, feat], dim=1).to(device)\n",
        "            out = self.act((self.weight(X)))\n",
        "            out = F.normalize(out, p=2.0, dim=1)\n",
        "            return out\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unknown aggregation type: {self.agg_type}\")\n",
        "\n",
        "\n",
        "class GraphSageLinkPred(nn.Module):\n",
        "    def __init__(self, num_layers_gnn: int, num_layers_nn: int, dim_in: int, dim_hidden: int, dim_hidden_nn: int, dim_out: int, p: float, agg_type: str):\n",
        "        super(GraphSageLinkPred, self).__init__()\n",
        "\n",
        "        self.num_layers_gnn = num_layers_gnn\n",
        "        self.num_layers_nn = num_layers_nn\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_hidden_nn = dim_hidden_nn\n",
        "        self.dim_out = dim_out\n",
        "        self.agg_type = agg_type\n",
        "        self.p = p\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for layer in range(num_layers_gnn):\n",
        "            self.layers.append(GraphSageLayer(\n",
        "                dim_in if layer == 0 else dim_hidden, dim_hidden, p, agg_type))\n",
        "\n",
        "        self.emb_layer = nn.Linear(dim_hidden, dim_out, bias=True, dtype=FLOAT_TYPE)\n",
        "\n",
        "        # node 의 순서는 보존이 되는 상황\n",
        "\n",
        "        layers_tmp = []\n",
        "        for layer in range(num_layers_nn):\n",
        "            linear_layer = nn.Linear(2 * dim_out if layer == 0 else dim_hidden_nn, dim_hidden_nn, bias=True, dtype=FLOAT_TYPE)\n",
        "            layers_tmp.append(linear_layer)\n",
        "            layers_tmp.append(nn.BatchNorm1d(dim_hidden_nn, dtype=FLOAT_TYPE))\n",
        "            layers_tmp.append(nn.Dropout(p=p, inplace=False))\n",
        "            layers_tmp.append(nn.ReLU())\n",
        "\n",
        "        self.layers_link_pred = nn.Sequential(*layers_tmp)\n",
        "\n",
        "        self.final_encoder = nn.Linear(dim_hidden_nn, 1, bias=True, dtype=FLOAT_TYPE)\n",
        "\n",
        "\n",
        "    def forward(self, feat: torch.Tensor, edges_tot: torch.Tensor, edges_train: torch.Tensor, degree: torch.Tensor) -> torch.Tensor:\n",
        "        x_in = feat\n",
        "        for layer in self.layers:\n",
        "            x_out = layer(x_in, edges_train, degree).to(device)\n",
        "            x_in = x_out\n",
        "\n",
        "        embedding = self.emb_layer(x_out)\n",
        "\n",
        "        concated = torch.cat((embedding[edges_tot[:, 0]], embedding[edges_tot[:, 1]]), dim=1)\n",
        "        out = self.layers_link_pred(concated).to(device)\n",
        "\n",
        "        final = self.final_encoder(out)\n",
        "        return final, embedding\n"
      ],
      "metadata": {
        "id": "F9A9XxBTdmxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, dim_in: int, dim_out: int, activation: lambda x: x, dropout: float = 0.5, alpha: float = 0.2):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.dropout = dropout\n",
        "        self.activation = activation\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty((dim_in, dim_out), dtype=FLOAT_TYPE))\n",
        "        nn.init.kaiming_uniform_(self.W.data)\n",
        "        self.a = nn.Parameter(torch.empty((2 * dim_out, 1), dtype=FLOAT_TYPE))\n",
        "        nn.init.kaiming_uniform_(self.a.data)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(negative_slope=self.alpha)\n",
        "\n",
        "    def forward(self, feat: torch.Tensor, edges: torch.Tensor) -> torch.Tensor:\n",
        "        # message passing (각각의 embedding을 먼저 linear transformation)\n",
        "        message_tensor = feat @ self.W\n",
        "\n",
        "        # attention weight 구하기\n",
        "        attention_src_tensor = message_tensor @ self.a[:self.dim_out, :]\n",
        "        attention_dst_tensor = message_tensor @ self.a[self.dim_out:, :]\n",
        "        attention_coef_tensor = self.leakyrelu(\n",
        "            attention_src_tensor[edges[:, 0]] + attention_dst_tensor[edges[:, 1]])  # e_uv\n",
        "        attention_coef_tensor = attention_coef_tensor - attention_coef_tensor.max() + 1e-6\n",
        "        exp_sum = torch.zeros((feat.shape[0], 1), dtype=FLOAT_TYPE).to(device)\n",
        "        exp_sum.index_add_(0, edges[:, 0], attention_coef_tensor.exp())\n",
        "        exp_sum = exp_sum + 1e-6\n",
        "        attention_weight_tensor = attention_coef_tensor.exp() / \\\n",
        "            exp_sum[edges[:, 0]]\n",
        "        attention_weight_tensor = F.dropout(\n",
        "            attention_weight_tensor, p=self.dropout, training=self.training, inplace=False)\n",
        "\n",
        "        neighbors_tensor = attention_weight_tensor * \\\n",
        "            message_tensor[edges[:, 1]]\n",
        "        weighted_sum_tensor = torch.zeros((feat.shape[0], self.dim_out), dtype=FLOAT_TYPE).to(\n",
        "            device).index_add_(0, edges[:, 0], neighbors_tensor)\n",
        "        out = self.activation(weighted_sum_tensor)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    '''\n",
        "    1. message passing\n",
        "    2. attention 구하기\n",
        "    3. attention으로 weighted sum 하고 linear transformation\n",
        "    '''\n",
        "\n",
        "    def __init__(self, feat_dim: int, emb_dim: int, num_layers_nn: int, dim_hidden: int, dropout: float = 0.5, alpha: float = 0.2, num_heads: int = 8) -> None:\n",
        "        super(GAT, self).__init__()\n",
        "        self.p = dropout\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attn_heads = nn.ModuleList(\n",
        "            [GATLayer(feat_dim, emb_dim, nn.ReLU(), dropout, alpha) for _ in range(num_heads)])\n",
        "\n",
        "        '''\n",
        "        Multihead Aggregation\n",
        "        - concatenation (current)\n",
        "        - mean, maxpool 등 적용해 볼수도\n",
        "        '''\n",
        "        layers_tmp = []\n",
        "        for i in range(num_layers_nn):\n",
        "            linear_layer = nn.Linear(\n",
        "                num_heads * emb_dim * 2 if i == 0 else dim_hidden, dim_hidden, bias=True, dtype=FLOAT_TYPE)\n",
        "            layers_tmp.append(linear_layer)\n",
        "            layers_tmp.append(nn.BatchNorm1d(dim_hidden, dtype=FLOAT_TYPE))\n",
        "            layers_tmp.append(nn.Dropout(p=self.p, inplace=False))\n",
        "            layers_tmp.append(nn.ReLU())\n",
        "        self.link_pred_layers = nn.Sequential(*layers_tmp)\n",
        "        self.final_encoder = nn.Linear(dim_hidden, 1, bias=True, dtype=FLOAT_TYPE)\n",
        "\n",
        "    def forward(self, feat: torch.Tensor, edges_tot: torch.Tensor, edges_train: torch.Tensor, degree: int):\n",
        "        feat = F.dropout(feat, p=0.5, training=self.training)\n",
        "        embeds = [head(feat, edges_train) for head in self.attn_heads]\n",
        "        res = torch.cat(embeds, dim=1).to(device)\n",
        "        res = F.dropout(res, p=0.5, training=self.training).to(device)\n",
        "\n",
        "        embeds_res = torch.tensor(\n",
        "            [embed.tolist() for embed in embeds], dtype=FLOAT_TYPE).to(device)\n",
        "\n",
        "        x_in = torch.cat([res[edges_tot[:, 0]], res[edges_tot[:, 1]]], dim=1).to(device)\n",
        "        out = self.link_pred_layers(x_in)\n",
        "\n",
        "        return self.final_encoder(out), embeds_res.mean(dim=0)"
      ],
      "metadata": {
        "id": "eY6F6zfUeAZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = node_features.to(device)\n",
        "edges_int = edges_int.to(device)\n",
        "train_indices_msg = train_indices_msg.to(device)\n",
        "train_indices_pred = train_indices_pred.to(device)\n",
        "val_indices = val_indices.to(device)"
      ],
      "metadata": {
        "id": "zwvc9xH7eBM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, feat: torch.Tensor, edge: torch.Tensor,\n",
        "          train_indices_msg: torch.Tensor, train_indices_pred: torch.Tensor, valid_indices: torch.Tensor, test_indices: torch.Tensor, lr: float, num_epochs: int, threshold: float):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    epoch_best = 0\n",
        "    best_valid_f1 = 0\n",
        "    list_valid_f1, list_loss = [], []\n",
        "\n",
        "    with tqdm(range(num_epochs), unit=\"epoch\", desc=\"Training\") as pbar:\n",
        "        pbar.clear()\n",
        "        for epoch in pbar:\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            degree = get_degree(feat.cpu(), edge.cpu()[train_indices_msg.cpu()]).to(device)\n",
        "            pred, _ = model.forward(feat, edge, edge[train_indices_msg], degree)  # [# of edges, 1] 을 반환\n",
        "            loss = F.binary_cross_entropy_with_logits(pred[train_indices_pred] + 1e-7, torch.ones(train_indices_pred.shape[0], 1).to(device))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # to prevent gradient exploding\n",
        "            optimizer.step()\n",
        "\n",
        "            list_loss.append(loss.item())\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                pred, _ = model(feat, edge, edge[train_indices_msg], degree)\n",
        "                train_f1 = binary_f1_score(pred[train_indices_pred].squeeze().sigmoid(), torch.ones(train_indices_pred.shape[0]).to(device), threshold=threshold)\n",
        "\n",
        "                degree = get_degree(feat, edge[torch.cat([train_indices_msg, train_indices_pred])]).to(device)\n",
        "                pred, _ = model(feat, edge, edge[torch.cat([train_indices_msg, train_indices_pred]).to(device)], degree)\n",
        "                valid_f1 = binary_f1_score(pred[valid_indices].squeeze().sigmoid(), torch.ones(valid_indices.shape[0]).to(device), threshold=threshold)\n",
        "\n",
        "                list_valid_f1.append(valid_f1.item())\n",
        "\n",
        "                if best_valid_f1 < valid_f1:\n",
        "                    best_valid_f1 = valid_f1\n",
        "                    epoch_best = epoch\n",
        "                    torch.save(model, './best_model.pt')\n",
        "\n",
        "                postfix_new = \", \".join([f\"f1_train: {train_f1:.7f} f1_val: {valid_f1:.7f} (best: {best_valid_f1:.7f} - Epoch {epoch_best})\",\n",
        "                                         f\"loss: {loss:.7f}\"])\n",
        "                pbar.set_postfix_str(postfix_new)\n",
        "\n",
        "    print(f\"Best Epoch: {epoch_best}\")\n",
        "    print(f\"Best Validation F1-score: {best_valid_f1}\")\n",
        "\n",
        "    best_model = torch.load(\"./best_model.pt\", map_location=device)\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        degree = get_degree(feat, edge[torch.cat([train_indices_msg, train_indices_pred, valid_indices])]).to(device)\n",
        "        pred, _ = model(feat, edge, edge[torch.cat([train_indices_msg, train_indices_pred, valid_indices]).to(device)], degree)\n",
        "        test_f1 = binary_f1_score(pred[test_indices].squeeze().sigmoid(), torch.ones(test_indices.shape[0]).to(device), threshold=threshold)\n",
        "\n",
        "    print(f\"Test F1-score: {test_f1}\")\n",
        "\n",
        "    return list_loss, list_valid_f1, test_f1"
      ],
      "metadata": {
        "id": "rXu5VcYoeC_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_train_history(num_epochs: int, list_train_loss: torch.Tensor, list_valid_f1: torch.Tensor, title: str):\n",
        "    X = np.arange(1, num_epochs + 1)\n",
        "    _, ax1 = plt.subplots()\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Train BCE Loss')\n",
        "    line1 = ax1.plot(X, list_train_loss, color='blue', label='Train')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('Valid F1-score')\n",
        "    line2 = ax2.plot(X, list_valid_f1, color='orange', label='Valid')\n",
        "\n",
        "    lines = line1 + line2\n",
        "    labels = [line.get_label() for line in lines]\n",
        "\n",
        "    ax1.legend(lines, labels, loc=\"upper left\", bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PhTPYqUIeF84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters_graphsage = {\n",
        "    \"num_layers\": 2,\n",
        "    \"num_layers_nn\": 3,\n",
        "    \"num_epochs\": 100,\n",
        "    \"dim_in\": 8,\n",
        "    \"dim_hidden\": 8,\n",
        "    \"dim_hidden_nn\": 16,\n",
        "    \"dim_out\": 2,\n",
        "    \"lr\": 0.001,\n",
        "    \"dropout\": 0.5,\n",
        "    \"agg_type\": \"mean\",\n",
        "    \"threshold\": 0.65\n",
        "}\n",
        "\n",
        "hyperparameters_gat = {\n",
        "    \"num_layers_nn\": 4,\n",
        "    \"num_epochs\": 100,\n",
        "    \"dim_in\": 8,\n",
        "    \"dim_hidden_nn\": 32,\n",
        "    \"dim_out\": 2,\n",
        "    \"lr\": 0.001,\n",
        "    \"dropout\": 0.5,\n",
        "    \"alpha\": 0.2,\n",
        "    \"num_heads\": 8,\n",
        "    \"threshold\": 0.65\n",
        "}"
      ],
      "metadata": {
        "id": "mFZpWRjWeMmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_graphsage(agg_type: str):\n",
        "    hyperparameters_graphsage['agg_type'] = agg_type\n",
        "\n",
        "    model = GraphSageLinkPred(\n",
        "        hyperparameters_graphsage['num_layers'],\n",
        "        hyperparameters_graphsage['num_layers_nn'],\n",
        "        hyperparameters_graphsage['dim_in'],\n",
        "        hyperparameters_graphsage['dim_hidden'],\n",
        "        hyperparameters_graphsage['dim_hidden_nn'],\n",
        "        hyperparameters_graphsage['dim_out'],\n",
        "        hyperparameters_graphsage['dropout'],\n",
        "        hyperparameters_graphsage['agg_type']\n",
        "    ).to(device)\n",
        "\n",
        "    list_loss, list_valid_f1, _ = train(\n",
        "        model, node_features, edges_int, train_indices_msg, train_indices_pred, val_indices, test_indices,\n",
        "        hyperparameters_graphsage['lr'], hyperparameters_graphsage['num_epochs'], hyperparameters_graphsage['threshold']\n",
        "    )\n",
        "\n",
        "    visualize_train_history(\n",
        "        hyperparameters_graphsage['num_epochs'], list_loss, list_valid_f1,\n",
        "        f\"GraphSAGE-{hyperparameters_graphsage['agg_type']}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def train_gat():\n",
        "    model = GAT(\n",
        "        node_features.shape[1],\n",
        "        hyperparameters_gat['dim_out'], hyperparameters_gat['num_layers_nn'],\n",
        "        hyperparameters_gat['dim_hidden_nn'], hyperparameters_gat['dropout'],\n",
        "        hyperparameters_gat['alpha'], hyperparameters_gat['num_heads']\n",
        "    ).to(device)\n",
        "\n",
        "    list_loss, list_valid_f1, _ = train(\n",
        "        model, node_features, edges_int, train_indices_msg, train_indices_pred,\n",
        "        val_indices, test_indices, hyperparameters_gat['lr'], hyperparameters_gat['num_epochs'],\n",
        "        hyperparameters_gat['threshold']\n",
        "    )\n",
        "\n",
        "    visualize_train_history(\n",
        "        hyperparameters_gat['num_epochs'], list_loss, list_valid_f1, 'GAT'\n",
        "    )"
      ],
      "metadata": {
        "id": "iEsCN3qGfT4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for agg in ['gcn', 'mean', 'maxpool']:\n",
        "    print(\"========================================================\\n\")\n",
        "    train_graphsage(agg)\n",
        "    print(\"\\n========================================================\")"
      ],
      "metadata": {
        "id": "InRa9t14hXF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gat()"
      ],
      "metadata": {
        "id": "wID3tr8Ihgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ITER = 10\n",
        "\n",
        "def get_mean_test_score(agg_type: str):\n",
        "    logs = []\n",
        "    for i in range(ITER):\n",
        "        if agg_type != None:\n",
        "            hyperparameters_graphsage['agg_type'] = agg_type\n",
        "            model = GraphSageLinkPred(\n",
        "            hyperparameters_graphsage['num_layers'],\n",
        "            hyperparameters_graphsage['num_layers_nn'],\n",
        "            hyperparameters_graphsage['dim_in'],\n",
        "            hyperparameters_graphsage['dim_hidden'],\n",
        "            hyperparameters_graphsage['dim_hidden_nn'],\n",
        "            hyperparameters_graphsage['dim_out'],\n",
        "            hyperparameters_graphsage['dropout'],\n",
        "            hyperparameters_graphsage['agg_type']\n",
        "            ).to(device)\n",
        "        else:\n",
        "            model = GAT(\n",
        "                node_features.shape[1],\n",
        "                hyperparameters_gat['dim_out'], hyperparameters_gat['num_layers_nn'],\n",
        "                hyperparameters_gat['dim_hidden_nn'], hyperparameters_gat['dropout'],\n",
        "                hyperparameters_gat['alpha'], hyperparameters_gat['num_heads']\n",
        "            ).to(device)\n",
        "\n",
        "        list_loss, list_valid_f1, test_f1 = train(\n",
        "            model, node_features, edges_int, train_indices_msg, train_indices_pred, val_indices, test_indices,\n",
        "            hyperparameters_graphsage['lr'], hyperparameters_graphsage['num_epochs'], hyperparameters_graphsage['threshold']\n",
        "        )\n",
        "\n",
        "        logs.append(test_f1.cpu().item())\n",
        "\n",
        "    return np.array(logs).mean()"
      ],
      "metadata": {
        "id": "qpGOBcVs9RXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = get_mean_test_score('gcn')\n",
        "print(f\"Mean Test F1-score of GraphSAGE-GCN: {score}\")"
      ],
      "metadata": {
        "id": "pJX915_-N6Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = get_mean_test_score('mean')\n",
        "print(f\"Mean Test F1-score of GraphSAGE-mean: {score}\")"
      ],
      "metadata": {
        "id": "8DCeJqGmOCS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = get_mean_test_score('maxpool')\n",
        "print(f\"Mean Test F1-score of GraphSAGE-maxpool: {score}\")"
      ],
      "metadata": {
        "id": "kRoNTMIdN9SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = get_mean_test_score(None)\n",
        "print(f\"Mean Test F1-score of GAT: {score}\")"
      ],
      "metadata": {
        "id": "eqeXf60YN_TT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}